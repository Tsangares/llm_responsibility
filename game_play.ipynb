{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: William Wyatt\n",
    "\n",
    "Purpose: To run LLMs against eachother in a stag-hare game to resolve how they compare to human trials and understand their reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "import yaml, json\n",
    "import pandas as pd\n",
    "import os, time, random\n",
    "from multiprocessing import Pool\n",
    "\n",
    "load_dotenv() #Load in a .env file with the variable OPENAI_API_KEY set to your OpenAI API key\n",
    "client = OpenAI()\n",
    "\n",
    "# Constants\n",
    "TRIALS = 1\n",
    "ROUNDS = 15\n",
    "SAVE=False\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "REVEAL_REASONING = False\n",
    "PROMPT_SYSTEM_PATH = './prompts/system_prompt_2.txt'\n",
    "PROMPT_PAIR_PATH = './prompts/play_for_pair_4_1.txt'\n",
    "PROMPT_SELF_PATH = './prompts/play_for_self_4_1.txt'\n",
    "RANDOM_AGENT=False\n",
    "PERSONA=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Below we setup personas along with the enums used by ChatGPT to get the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Personas\n",
    "with open('./personas.json') as f:\n",
    "    pesonas = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEvent(BaseModel):\n",
    "    stag: bool\n",
    "    reasoning: str\n",
    "\n",
    "class History(BaseModel):\n",
    "    name: str\n",
    "    stag: bool\n",
    "    period: int\n",
    "    payoff: int\n",
    "    reasoning: str\n",
    "\n",
    "class Games(BaseModel):\n",
    "    event: List[GameEvent] = Field(default_factory=list)\n",
    "\n",
    "class GameHistory(BaseModel):\n",
    "    history: List[History] = Field(default_factory=list)\n",
    "\n",
    "def run_agent_decision(system_prompt, game_prompt, name=None, model=MODEL, response_format=GameEvent):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": game_prompt}\n",
    "    ]\n",
    "    try:\n",
    "        response = client.beta.chat.completions.parse(\n",
    "            model=model, \n",
    "            messages=messages,\n",
    "            response_format=response_format\n",
    "            )\n",
    "        results = response_format.model_validate_json(response.choices[0].message.content).model_dump()\n",
    "        #print(f\"{name} ({results['stag']}): {results['reasoning']}\")\n",
    "        results['name'] = name\n",
    "        results['model'] = model\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Agent - p=50% Bernoulli\n",
    "def get_random_decision():\n",
    "    choice = {\"stag\": random.choice([True, False]), \"reasoning\": \"random\"}\n",
    "    #print(f\"Chose {choice['stag']} randomly\")\n",
    "    return choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Agent - p=87.5% Bernoulli\n",
    "def get_mixed_strat():\n",
    "    rng = random.random()\n",
    "    if rng < 7/8:\n",
    "        choice = {\"stag\": True, \"reasoning\": \"mixed\"}\n",
    "    else:\n",
    "        choice = {\"stag\": False, \"reasoning\": \"mixed\"}\n",
    "    #print(f\"Chose {choice['stag']} randomly\")\n",
    "    return choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game Details\n",
    "This the bulk of the code. Describing the game functionality. Running a sessions with rounds, a repeated game. Along with the logic for the LLMs to play the game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets game parameters\n",
    "PAYOFF_HIGH = 9\n",
    "PAYOFF_MID  = 8\n",
    "PAYOFF_LOW  = 1\n",
    "\n",
    "# Gets prompts\n",
    "PROMPT_SYSTEM = open(PROMPT_SYSTEM_PATH,'r').read()\n",
    "\n",
    "# Sets personas into system prompt\n",
    "if PERSONA is not None:\n",
    "    PROMPT_SYSTEM = PROMPT_SYSTEM.replace(\"PERSONA\",PERSONA)\n",
    "\n",
    "# Get user prompts\n",
    "PROMPT_PAIR = open(PROMPT_PAIR_PATH,'r').read()\n",
    "PROMPT_SELF = open(PROMPT_SELF_PATH,'r').read()\n",
    "\n",
    "# Run Game\n",
    "def calculate_payoff(round_events):\n",
    "    payoffs = {}\n",
    "    name_1,name_2 = list(round_events.keys())\n",
    "    decision_1 = round_events[name_1]['stag']\n",
    "    decision_2 = round_events[name_2]['stag']\n",
    "    if decision_1 and decision_2:\n",
    "        payoffs[name_1] = PAYOFF_HIGH\n",
    "        payoffs[name_2] = PAYOFF_HIGH\n",
    "    elif decision_1 and not decision_2:\n",
    "        payoffs[name_1] = PAYOFF_LOW\n",
    "        payoffs[name_2] = PAYOFF_MID\n",
    "    elif not decision_1 and decision_2:\n",
    "        payoffs[name_1] = PAYOFF_MID\n",
    "        payoffs[name_2] = PAYOFF_LOW\n",
    "    else:\n",
    "        payoffs[name_1] = PAYOFF_MID\n",
    "        payoffs[name_2] = PAYOFF_MID\n",
    "    return payoffs\n",
    "\n",
    "# Summarize game results into a string to give back to the LLM agent's user prompt\n",
    "def get_game_summary(name, histories, reveal_reasoning=REVEAL_REASONING):\n",
    "    if len(histories) == 0:\n",
    "        return \"\"\n",
    "    summary = \"# Previous Game Summaries\\n\"\n",
    "    for round, round_events in enumerate(histories):\n",
    "        summary += f\"## Round {round}\\n\"\n",
    "        for _, event in round_events.items():\n",
    "            _name = \"You\" if event.name == name else event.name\n",
    "            _reasoning = event.reasoning\n",
    "            _selection = \"Stag\" if event.stag else \"Hare\"\n",
    "            summary += f\"- {_name} selected {_selection} and received a payoff of {event.payoff}\\n\"\n",
    "            if reveal_reasoning and event.name == name:\n",
    "                summary += f\"  - Your Reasoning: {_reasoning}\\n\"\n",
    "        summary += \"\\n\"\n",
    "    #print(summary)\n",
    "    return summary\n",
    "\n",
    "# Run a number of rounds of the game, in one entire session.\n",
    "# - rounds: number of rounds to play\n",
    "# - player_names: list of player names\n",
    "# - system_prompt: the system prompt to use\n",
    "# - prompt: the user prompt to use\n",
    "# - strat: Is not None will use a custom strategy instead of LLM vs LLM\n",
    "def run_session(rounds, player_names, system_prompt, prompt, strat=None):\n",
    "    histories = []\n",
    "    for round in range(rounds):\n",
    "        round_events = {}\n",
    "        decisions = {}\n",
    "        for i,name in enumerate(player_names):\n",
    "            game_summary = get_game_summary(name, histories)\n",
    "            #print(prompt + game_summary)\n",
    "            if strat is not None and i > 0:\n",
    "                decisions[name] = strat()\n",
    "            else:\n",
    "                decisions[name] = run_agent_decision(system_prompt, prompt + game_summary, name)\n",
    "        \n",
    "        payoffs = calculate_payoff(decisions)\n",
    "        \n",
    "        for name, decision_result in decisions.items():\n",
    "            event = History(name=name, stag=decision_result['stag'], period=round, payoff=payoffs[name], reasoning=decision_result['reasoning'])\n",
    "            round_events[name] = event\n",
    "            \n",
    "        histories.append(round_events)\n",
    "    return histories\n",
    "\n",
    "# Returns a dataframe given a list of histories enums\n",
    "def get_df_from_history(all_histories):\n",
    "    data = []\n",
    "    print(all_histories)\n",
    "    for round, histories in enumerate(all_histories):\n",
    "        for name,history in histories.items():\n",
    "            data.append({\n",
    "                'player': name,\n",
    "                'period': history.period,\n",
    "                'stag': history.stag,\n",
    "                'payoff': history.payoff\n",
    "            })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Wrapper function for the play for pair game\n",
    "# - return_df: If True returns a dataframe, otherwise returns the list of History Objects\n",
    "def run_session_play_for_pair(rounds=15, strat=None, return_df=True):\n",
    "    if RANDOM_AGENT == True:\n",
    "        strat = get_random_decision\n",
    "    player_names = ['Team1','Team2']\n",
    "    system_prompt = PROMPT_SYSTEM\n",
    "    prompt = PROMPT_PAIR\n",
    "    \n",
    "    all_histories = run_session(rounds, player_names, system_prompt, prompt, strat)\n",
    "    \n",
    "    if return_df:\n",
    "        return get_df_from_history(all_histories)\n",
    "    else:\n",
    "        return all_histories\n",
    "\n",
    "# Wrapper function for the play for self game\n",
    "# - return_df: If True returns a dataframe, otherwise returns the list of History Objects        \n",
    "def run_session_play_for_self(rounds=15, strat=None, return_df=True):\n",
    "    if RANDOM_AGENT == True:\n",
    "        strat = get_random_decision\n",
    "    player_names = ['Player1', 'Player2']\n",
    "    system_prompt = PROMPT_SYSTEM\n",
    "    prompt = PROMPT_SELF\n",
    "    \n",
    "    all_histories = run_session(rounds, player_names, system_prompt, prompt, strat)\n",
    "    if return_df:\n",
    "        return get_df_from_history(all_histories)\n",
    "    else:\n",
    "        return all_histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "The following code blocks are used for a brief test of the LLMs.\n",
    "\n",
    "Additionally, this section is where new functions can be sandboxed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one game just to test the code is working\n",
    "pp_test = run_session_play_for_pair(1, return_df=False)\n",
    "ps_test = run_session_play_for_self(1, return_df=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra function to ask the LLM about its actions.\n",
    "def get_conclusion(histories):\n",
    "    query = \"You are Team 1. Why did you choose this original decision despite it being more risky? Even though you were responsible for another person? Please provide 500 words.\"\n",
    "    \n",
    "    summary = get_game_summary(\"Team1\", histories, reveal_reasoning=True)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": PROMPT_SYSTEM},\n",
    "        {\"role\": \"assistant\", \"content\": PROMPT_PAIR +summary},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "]\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL, \n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "##print(get_conclusion(pp_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Game\n",
    "The following blocks of code will run the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multithreadding to run play for pair games\n",
    "with Pool(20) as p:\n",
    "    results_pair = p.map(run_session_play_for_pair, [ROUNDS]*TRIALS)\n",
    "pp_df = pd.concat(results_pair)\n",
    "print(pp_df['stag'].mean())\n",
    "pp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutlithreadding to run play for self games\n",
    "with Pool(20) as p:\n",
    "    results_self = p.map(run_session_play_for_self, [ROUNDS]*TRIALS)\n",
    "ps_df = pd.concat(results_self)\n",
    "print(ps_df['stag'].mean())\n",
    "ps_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving\n",
    "Save the game data and the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the metadata of the game and the data resulted from the games to file. \n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "if SAVE or TRIALS > 10:\n",
    "\n",
    "    path_info = {\n",
    "        'system path': PROMPT_SYSTEM_PATH,\n",
    "        'pair path': PROMPT_PAIR_PATH,\n",
    "        'self path': PROMPT_SELF_PATH\n",
    "    }\n",
    "    metadata = {\n",
    "        'time': timestamp,\n",
    "        'SAVE': SAVE,\n",
    "        'REVEAL_REASONING': REVEAL_REASONING,\n",
    "        'PERSONA': PERSONA,\n",
    "        'number of rounds': TRIALS,\n",
    "        'rounds per trail': ROUNDS,\n",
    "        'number ow rows': len(pp_df),\n",
    "        'random agent': RANDOM_AGENT,\n",
    "        'model used': MODEL,\n",
    "        'game model': vars(GameEvent)['__annotations__'],\n",
    "        'path_info': path_info,\n",
    "        'prompts':\n",
    "        {\n",
    "            'system': PROMPT_SYSTEM,\n",
    "            'pair': PROMPT_PAIR,\n",
    "            'self': PROMPT_SELF\n",
    "        }\n",
    "        \n",
    "    }\n",
    "    filename = f'./dist/notes_{int(timestamp)}.yaml'\n",
    "\n",
    "    # Write metadata in yaml format\n",
    "    with open(filename, 'w') as file:\n",
    "        documents = yaml.dump(metadata, file)\n",
    "        \n",
    "    pp_df.to_csv(f'./dist/pp_df_{int(timestamp)}.csv', index=False)\n",
    "    ps_df.to_csv(f'./dist/ps_df_{int(timestamp)}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "A quick analysis of the aggregate choices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Percentages\n",
    "\n",
    "# Play-for-Pair\n",
    "pp_total_decisions = len(pp_df)\n",
    "pp_yes_decisions = pp_df[pp_df['stag'] == True].shape[0]\n",
    "pp_percentage_yes = (pp_yes_decisions / pp_total_decisions) * 100\n",
    "\n",
    "# Play-for-Self\n",
    "ps_total_decisions = len(ps_df)\n",
    "ps_yes_decisions = ps_df[ps_df['stag'] == True].shape[0]\n",
    "ps_percentage_yes = (ps_yes_decisions / ps_total_decisions) * 100\n",
    "\n",
    "# Your experiment's percentages\n",
    "your_percentages = [pp_percentage_yes, ps_percentage_yes]\n",
    "\n",
    "# Papers Results\n",
    "paper_pp_percentage = 43.9\n",
    "paper_ps_percentage = 61.2\n",
    "\n",
    "paper_percentages = [paper_pp_percentage, paper_ps_percentage]\n",
    "\n",
    "# Data for plotting\n",
    "game_types = ['Play-for-Pair', 'Play-for-Self']\n",
    "x = np.arange(len(game_types))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "# Create the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "rects1 = plt.bar(x - width/2, your_percentages, width, label='Your Experiment', color=['blue'])\n",
    "rects2 = plt.bar(x + width/2, paper_percentages, width, label='Charness & Jackson (2009)', color=['green'])\n",
    "\n",
    "# Add percentage labels on top of the bars\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        plt.annotate(f'{height:.2f}%',\n",
    "                     xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                     xytext=(0, 3),  # 3 points vertical offset\n",
    "                     textcoords=\"offset points\",\n",
    "                     ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "# Chart formatting\n",
    "plt.title('Percentage of \"Yes\" Decisions by Game Type')\n",
    "plt.ylabel('Percentage of \"Yes\" Decisions')\n",
    "plt.xticks(x, game_types)\n",
    "plt.ylim(0, 100)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of \"Yes\" decisions per player for Play-for-Pair\n",
    "pp_player_percentages = pp_df.groupby('player')['stag'].mean() * 100\n",
    "\n",
    "# Calculate percentage of \"Yes\" decisions per player for Play-for-Self\n",
    "ps_player_percentages = ps_df.groupby('player')['stag'].mean() * 100\n",
    "\n",
    "# Plotting for Play-for-Pair\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "pp_player_percentages.plot(kind='bar', color='blue')\n",
    "plt.title('Player \"Yes\" Decision Percentage - Play-for-Pair')\n",
    "plt.xlabel('Player')\n",
    "plt.ylabel('Percentage of \"Yes\" Decisions')\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# Plotting for Play-for-Self\n",
    "plt.subplot(1, 2, 2)\n",
    "ps_player_percentages.plot(kind='bar', color='green')\n",
    "plt.title('Player \"Yes\" Decision Percentage - Play-for-Self')\n",
    "plt.xlabel('Player')\n",
    "plt.ylabel('Percentage of \"Yes\" Decisions')\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pp_df['payoff'].value_counts())\n",
    "print(ps_df['payoff'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
